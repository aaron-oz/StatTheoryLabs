---
title: "Getting Started on the Project - Nonparametric Tests and Their Power"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(learnrhash)
library(tidyverse)
library(gradethis)
library(broom)
tutorial_options(
  # use gradethis for checking
  exercise.checker = gradethis::grade_learnr
  )
knitr::opts_chunk$set(echo = FALSE)
set.seed(113877)

```

## Getting Started - Some New Packages

The `tidyverse` is a suite of packages that will help us work with data. Depending on which test your project is focused on, you may need to use another package that has the test implemented.

```{r packages, exercise = T}
library(tidyverse)
```


## Simulating A Simple Dataset 

The `head` function shows the first 6 rows of the dataframe, so that we can peak and see what our new data looks like. The `dim` function tells us the dimensions of the object. This can come in handy for checking that we have something with the dimensions we expect (like here we said `n=100` so we should end up with something that has 100 rows).

**Generate an explanatory variable**

```{r simDat, exercise = T}
simulated_data <- data.frame(
  x = rnorm(n = 100, mean = 5, sd = 1)
)
head(simulated_data)
dim(simulated_data)
```

**Generate the response variable based on the explanatory variable**

The `%>%` operator is called the **piping** operator. It takes the output of the previous expression and pipes it into the first argument of the function in the following one. 
To continue our analogy with mathematical functions, `x %>% f(y)` is equivalent to `f(x, y)`.

**A note on piping:** Note that we can read these two lines of code as the following: *"Take the `simulated_data` dataset and **pipe** it into the `mutate` function. 
Mutate the `simulated_data` data set by creating a new variable called `y` that is another random draw from a normal distribution.

Then assign the resulting dataset to the object called `simulated_data`, i.e. overwrite the old `simulated_data` dataset with the new one containing the new variable."*


```{r simDat2, exercise = T}
simulated_data <- simulated_data %>%
  mutate(
    y =  rnorm(n = 100, mean = 5, sd = .74)
  )
head(simulated_data)
dim(simulated_data)
```

**Plot our data**

Up until now, we have been making simple plots, but now we will want more professional looking graphs.

With `ggplot()`:

- The first argument is always the dataset. 
- Next, you provide the variables from the dataset to be assigned to `aes`thetic elements of the plot, e.g. the x and the y axes. 
- Finally, you use another layer, separated by a `+` to specify the `geom`etric object for the plot. Since we want to scatterplot, we use `geom_point()`.

For instance, if you wanted to visualize the scatterplot below using a line graph, you would replace `geom_point()` with `geom_line()`.


```{r plot, exercise = T}
ggplot(simulated_data, aes(x, y)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Informative X label here", y = "Informative Y label here") +
  ggtitle("Informative title here") +
  theme_minimal(base_size = 20)

ggplot(simulated_data, aes(x)) + 
  geom_histogram() +
  labs(x = "Informative X label here") +
  ggtitle("Informative title here") +
  theme_minimal(base_size = 20)

ggplot(simulated_data, aes(y)) + 
  geom_histogram() +
  labs(x = "Informative X label here") +
  ggtitle("Informative title here") +
  theme_minimal(base_size = 20)
```


## Function - Simulate Data

We will want to do this many times in a simulation study, so we should wrap this code into functions that we can call multiple times. 

```{r func, exercise = T}
sim.dataset <- function(n, mean_vals, sd_vals) {
  # Simulate X variable:
  simulated_data <- data.frame(
    x = rnorm(n = n, mean = mean_vals[1], sd = sd_vals[1])
  )
  # Simulate Y variable:
  simulated_data <- simulated_data %>%
    mutate(
      y = rnorm(n = n, mean = mean_vals[2], sd = sd_vals[2])
    )
  # Return dataset
  return(simulated_data)
}

## Note: your sim.dataset might look different, perhaps you won't want rnorm for example
```

Let's try it out! We call the function we just wrote, plugging in the expected parameters, and save the output to `test_dataset`.

```{r testF, exercise = T}
test_dataset <- sim.dataset(n = 150, mean_vals = c(0,5), sd_vals = c(1,1))

ggplot(test_dataset, aes(x)) + 
  geom_histogram()+
  labs(x = "Informative X label here") +
  ggtitle("Informative title here") +
  theme_minimal(base_size = 20)

ggplot(test_dataset, aes(y)) + 
  geom_histogram()+
  labs(x = "Informative X label here") +
  ggtitle("Informative title here") +
  theme_minimal(base_size = 20)

```


## Many Simulations via Loops

We'll need to create a dataframe to store our results.

```{r storage, exercise = T}
power.val <- data.frame(
  pval_param = rep(NA, 500),
  pval_nonparam = rep(NA, 500)
  )
head(power.val)
dim(power.val)
```


Here we have chosen our $\alpha$ to be 0.05. If the p-value for the coefficient is less than $\alpha$ we will reject the null hypothesis.

```{r firstSim, echo = T, eval = F}
significance_value <- 0.05

for (dataset.i in 1:500) {
  simulated_df <- sim.dataset(n = 150, mean_vals = c(0,5), sd_vals = c(1,1))
  res_param = ## replace with your parametric test
  res_nonparam = ## replace with your nonparametric test
  power.val$pval_param[dataset.i] <- #res_param's pvalue
                                          < significance_value
  power.val$pval_nonparam[dataset.i] <- #res_nonparam's pvalue
                                            < significance_value
}
table(power.val$pval_param)
table(power.val$pval_nonparam)
```

We can get a quick sense of how many times (out of 500) each test was rejected (TRUE).

## Function - Get Power

```{r funcP, echo = T, eval = F}
simulate.power <- function(n.sims, n, mean_vals, sd_vals) {
  
  # Dataframe to hold results:
  power.val <- data.frame(
  pval_param = rep(NA, n.sims),
  pval_nonparam = rep(NA, n.sims)
  )
  
  # Simulating many tests:
  for (dataset.i in 1:n.sims) {
    simulated_df <- sim.dataset(n = n, mean_vals = mean_vals, sd_vals = sd_vals)
    res_param <- ## replace with your parametric test
    res_nonparam <- ## replace with your nonparametric test
    power.val$pval_param[dataset.i] <- #res_param's pvalue
                                          < significance_value
    power.val$pval_nonparam[dataset.i] <- #res_nonparam's pvalue
                                            < significance_value
  }
  
  # Aggregating into summary 
  power.val <- summarise(power.val,
                          param.power = mean(pval_param),
                          nonparam.power = mean(pval_nonparam))
  
  # Returning a summary dataframe
  return(power.val)
}
```

To call this function:

```{r, power, exercise = T}
simulate.power(n.sims = 500,
               n = 100, 
               mean_vals = c(0,5), sd_vals = c(1,1))
```


## Outline of a Simulation Study - Changing One Parameter

There is inherent randomness in a simulation study. This is not ideal as your answers would have to change each time you reran code. By "setting a seed" at the start of your study you tell the random number generator where to start in its sequence of random numbers (so you'll get the same answer each time you rerun the code in the same order). You can think of the number you pick for `set.seed()` as a bookmark for the sampling algorithm. Setting the seed also ensures others can reproduce your work.

Talking more about this is beyond the scope of the class, but if you are interested, I point you to Kellie Ottoboni's cool work on [sampling in R](http://www.kellieottoboni.com/posts/2019/01/random-problems-with-r/).

Be patient. This has to go through 500*4 linear regressions.

```{r outline, exercise = T}
set.seed(721226) ## set seed so your findings are reproducible
num_reps <- 500 ## turn into a variable so we can easily change it
sample_size <- 100 ## turn into a variable so we can easily change it
b1.truth <- c(0.1, 0.2, 0.5, 1) ## possible scenarios
num_levels <- length(b1.truth) ## how many scenarios
## create a space to store power from each scenario
sim_study_output <- data.frame(
  b0.power = rep(NA, num_levels),
  b1.power = rep(NA, num_levels),
  b1.truth = b1.truth
)
```

```{r outline1b-setup}
set.seed(721226) ## set seed so your findings are reproducible
num_reps <- 500 ## turn into a variable so we can easily change it
sample_size <- 100 ## turn into a variable so we can easily change it
b1.truth <- c(0.1, 0.2, 0.5, 1) ## possible scenarios
num_levels <- length(b1.truth) ## how many scenarios
## create a space to store power from each scenario
sim_study_output <- data.frame(
  b0.power = rep(NA, num_levels),
  b1.power = rep(NA, num_levels),
  b1.truth = b1.truth
)
```

```{r, outline1b, exercise = T}
for (i in 1:num_levels) {
  sim_study_output[i, 1:2] <- simulate.power(
    n.sims = num_reps,
    n = sample_size,
    b0 = 2.80, b1 = b1.truth[i], residual_se = .74
  )
  ## only overwrite the first two columns
  ## same scenario except each time we change b1
}
sim_study_output
ggplot(sim_study_output, aes(x = b1.truth, y = b1.power)) +
  geom_point() +
  geom_line() +
  labs(x = "Slope Coefficient", y = "Power (alpha = 0.05)") +
  ggtitle("Power When Varying Effect Size") +
  theme_minimal(base_size = 20)
```

We can plot the power as a function of the slope coefficient.

## Outline of a Simulation Study - Changing Two Parameters

What if we want to change two parameters. Now we need all combinations of pairs of parameters. This can add up quickly so be strategic about the combinations you are interested in.

```{r outline2, exercise = T}
set.seed(721226) ## set seed so your findings are reproducible
num_reps <- 500 ## turn into a variable so we can easily change it
sample_size <- c(50, 100, 150) ## possible scenarios
b1.truth <- c(0.1, 0.2, 0.5, 1) ## possible scenarios
num_levels <- length(b1.truth) * length(sample_size) ## how many scenarios
all_scenarios <- expand.grid(sample_size = sample_size, b1.truth = b1.truth)
## get all possible combinations of these two parameters
head(all_scenarios)
dim(all_scenarios)
## create a space to store power from each scenario
sim_study_output <- data.frame(
  b0.power = rep(NA, num_levels),
  b1.power = rep(NA, num_levels),
  b1.truth = as.factor(b1.truth),
  sample_size = sample_size
)
## the as.factor lets R know that we want this to be a category, useful for plotting later
```

```{r, outline2b-setup}
set.seed(721226) ## set seed so your findings are reproducible
num_reps <- 500 ## turn into a variable so we can easily change it
sample_size <- c(50, 100, 150) ## possible scenarios
b1.truth <- c(0.1, 0.2, 0.5, 1) ## possible scenarios
num_levels <- length(b1.truth) * length(sample_size) ## how many scenarios
all_scenarios <- expand.grid(sample_size = sample_size, b1.truth = b1.truth)
## create a space to store power from each scenario
sim_study_output <- data.frame(
  b0.power = rep(NA, num_levels),
  b1.power = rep(NA, num_levels),
  b1.truth = as.factor(b1.truth),
  sample_size = sample_size
)
```

Now we are interested in the relationship between sample size and power for different scenarios. We can plot the power as a function of the sample size and denote the scenarios with different colors.



```{r, outline2b, exercise = T}
for (i in 1:num_levels) {
  sim_study_output[i, 1:2] <- simulate.power(
    n.sims = num_reps,
    n = all_scenarios$sample_size[i],
    b0 = 2.80, b1 = all_scenarios$b1.truth[i], residual_se = .74
  )
  ## only overwrite the first two columns
  ## same scenario except each time we change b1 and sample size
}
sim_study_output
ggplot(sim_study_output, aes(x = sample_size, y = b1.power, col = b1.truth)) +
  geom_point() +
  geom_line() +
  labs(x = "Sample Size", y = "Power (alpha = 0.05)") +
  ggtitle("Power When Varying Effect Size \n and Sample Size") +
  theme_minimal(base_size = 20)
## the \n adds a line break in the title
```

## Outline of a Simulation Study - Changing Three Parameters

What if we want to change three parameters? Now we need all combinations of triplets of parameters. Again this can add up quickly so be strategic about the combinations you are interested in. To do 500 replications for 27 scenarios takes about a minute and a half. Here we will cut down the number of replications to 100, so we have to wait less than 30 seconds.

```{r outline3, exercise = T}
set.seed(721226) ## set seed so your findings are reproducible
num_reps <- 100 ## turn into a variable so we can easily change it
sample_size <- c(50, 100, 150) ## possible scenarios
b1.truth <- c(0.1, 0.5, 1) ## possible scenarios
residual_se <- c(0.2, 0.5, 0.75)
num_levels <- length(b1.truth) * length(sample_size) * length(residual_se) ## how many scenarios
all_scenarios <- expand.grid(sample_size = sample_size, b1.truth = b1.truth, residual_se = residual_se)
## get all possible combinations of these two parameters
head(all_scenarios)
dim(all_scenarios)
## create a space to store power from each scenario
sim_study_output <- data.frame(
  b0.power = rep(NA, num_levels),
  b1.power = rep(NA, num_levels),
  b1.truth = as.factor(b1.truth),
  sample_size = sample_size,
  residual_se = as.factor(residual_se)
)
## the as.factor lets R know that we want this to be a category, useful for plotting later
```

```{r outline3b-setup}
set.seed(721226) ## set seed so your findings are reproducible
num_reps <- 500 ## turn into a variable so we can easily change it
sample_size <- c(50, 100, 150) ## possible scenarios
b1.truth <- c(0.1, 0.5, 1) ## possible scenarios
residual_se <- c(0.2, 0.5, 0.75)
num_levels <- length(b1.truth) * length(sample_size) * length(residual_se) ## how many scenarios
all_scenarios <- expand.grid(sample_size = sample_size, b1.truth = b1.truth, residual_se = residual_se)
## create a space to store power from each scenario
sim_study_output <- data.frame(
  b0.power = rep(NA, num_levels),
  b1.power = rep(NA, num_levels),
  b1.truth = as.factor(b1.truth),
  sample_size = sample_size,
  residual_se = as.factor(residual_se)
)
## the as.factor lets R know that we want this to be a category, useful for plotting later
```


```{r outline3b, exercise = T}
for (i in 1:num_levels) {
  sim_study_output[i, 1:2] <- simulate.power(
    n.sims = num_reps,
    n = all_scenarios$sample_size[i],
    b0 = 2.80, b1 = all_scenarios$b1.truth[i], residual_se = all_scenarios$residual_se[i]
  )
  ## only overwrite the first two columns
  ## same scenario except each time we change b1, sample size, and residual standard error
}
sim_study_output
ggplot(sim_study_output, aes(x = sample_size, y = b1.power, col = b1.truth)) +
  geom_point() +
  geom_line() +
  facet_wrap(~residual_se) +
  labs(x = "Sample Size", y = "Power (alpha = 0.05)") +
  ggtitle("Power When Varying Effect Size \n and Sample Size") +
  theme_minimal(base_size = 20)
## the \n adds a line break in the title
## the facet_wrap makes a different plot of the same form for each of the specified categories
```

Now we are interested in the relationship between sample size and power for different scenarios that vary on two other parameters. We can plot the power as a function of the sample size and denote the scenarios with different colors and across multiple panels or "facets".

## Project Preview

Each project group will use the same number of replications, the same values the intercept for the regression line, the same sample sizes (multiple, across x-axis in result plots), the same slope coefficients for the regression line (multiple, color in result plots) and the same number of levels of misspecification (multiple, facets in result plots). You will be responsible for specifying what those misspecification levels look like. The residual standard error will take different forms depending on the misspecification scenario you are in.

* * *

This lab uses materials from [A. Hales](https://osf.io/rzbyj/) and various Open Intro labs and was adapted and learnr-ified by S. Stoudt.