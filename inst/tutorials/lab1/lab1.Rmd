---
title: "Estimation"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(learnrhash)
library(tidyverse)
library(gradethis)
tutorial_options(
  # use gradethis for checking
  exercise.checker = gradethis::grade_learnr
  )
knitr::opts_chunk$set(echo = FALSE)
set.seed(113877)


makeTarget <- function(){
  t = seq(0, 2*pi, length = 1000)

# Circle centered at (0,0) with radius 2
coords = t(rbind(sin(t)*2, cos(t)*2))   	

  plot(coords, type = 'l', xlab = "", ylab = "", xaxt = 'n', yaxt = 'n', xlim = c(-3, 3), ylim = c(-3, 3))

# Circle centered at (0,0) with radius 1.75
coords.out = t(rbind(sin(t)*1.75, cos(t)*1.75))	  
lines(coords.out, type = 'l')                   

# Circle centered at (0,0) with radius 1
c.1 = t(rbind(sin(t)*1, cos(t)*1))	              
lines(c.1, type = 'l')

# Circle centered at (0,0) with radius .75
c.2 = t(rbind(sin(t)*.75, cos(t)*.75))	          
lines(c.2, type = 'l')

# Circle centered at (0,0) with radius .25
coords.in = t(rbind(sin(t)*.25, cos(t)*.25))	  
lines(coords.in, type = 'l')

# Circle centered at (0,0) with radius .1
coords.in.in = t(rbind(sin(t)*.1, cos(t)*.1))	  

# Color in center circle green
polygon(coords.in.in, col = 'green')	
}

```

## Bias & Variance

**Part I**

Recall from lecture:

- $\hat{\theta}$ is a random variable.

It is a function of $X_1, ..., X_n$ and will therefore produce a different estimate from each different sample.

- As a RV, $\hat{\theta}$ has a sampling distribution.

This is the distribution of values of $\hat{\theta}$ produced from all possible samples of the same size from the population.

- We would like $\hat{\theta}$ to be unbiased.

i.e. The sampling distribution of $\hat{\theta}$ is centered at $\theta$

- We would like $\hat{\theta}$ to have small variance.

i.e. The sampling distribution of $\hat{\theta}$ is concentrated as much as possible around \theta

We are going to simulate data to provide estimates of $\theta$ that have varying properties (low bias/low variance, high bias/high variance, low bias/high variance, high bias/low variance).

To begin, we must first create the target. Let's make a function to do this for us. Note in your lab report you will need to include this function in your Rmd before calling it.


```{r, makeTarget, exercise = T}
## no arguments are necessary
makeTarget <- function() {
  t <- seq(0, 2 * pi, length = 1000)

  # Circle centered at (0,0) with radius 2
  coords <- t(rbind(sin(t) * 2, cos(t) * 2))

  plot(coords, type = "l", xlab = "", ylab = "", xaxt = "n", yaxt = "n", xlim = c(-3, 3), ylim = c(-3, 3))

  # Circle centered at (0,0) with radius 1.75
  coords.out <- t(rbind(sin(t) * 1.75, cos(t) * 1.75))
  lines(coords.out, type = "l")

  # Circle centered at (0,0) with radius 1
  c.1 <- t(rbind(sin(t) * 1, cos(t) * 1))
  lines(c.1, type = "l")

  # Circle centered at (0,0) with radius .75
  c.2 <- t(rbind(sin(t) * .75, cos(t) * .75))
  lines(c.2, type = "l")

  # Circle centered at (0,0) with radius .25
  coords.in <- t(rbind(sin(t) * .25, cos(t) * .25))
  lines(coords.in, type = "l")

  # Circle centered at (0,0) with radius .1
  coords.in.in <- t(rbind(sin(t) * .1, cos(t) * .1))

  # Color in center circle green
  polygon(coords.in.in, col = "green")
}

makeTarget()
```

Notice that the outer circle of the target has a radius of 2. When simulating data, we are going to make draws from two separate Uniform distributions. One that represents our x-coordinate and one that represents our y-coordinate. If we want to produce estimates centered on the bullseye (low bias) that could occur at the far edges of the target (high variance), then one way to achieve this is to simulate data from two Uniform distributions with parameters near -2 and 2. 

## Simulating high variance & low bias

```{r, hvlb, exercise = T}
# Initiate vectors for (x, y) coordinates
x.ran_HVLB <- y.ran_HVLB <- NULL

## start with a blank target
makeTarget()

for (i in 1:10) {
  # Randomly draw a dart from a Uniform(-2, 2).
  # Here, left of the bullseye is negative and right of the bullseye is positive.
  x.ran_HVLB[i] <- runif(1, -2, 2)

  # Randomly draw a dart from a Uniform(-2, 2).
  # Here, above the bullseye is positive and below the bullseye is negative.
  y.ran_HVLB[i] <- runif(1, -2, 2)

  # Draw the dart at the (x,y) coordinates from the previous draw
  points(x.ran_HVLB, y.ran_HVLB, pch = "X", cex = 2, col = "black")

}
```

You now have 10 black Xâ€™s on your target that represent observations with low bias and high variance. We can calculate the bias and variance as follows:

```{r, calcBV, exercise = T}
# Proxy for the bias of throws.
Bias.black <- mean(x.ran_HVLB) + mean(y.ran_HVLB)
# Proxy for the variance of throws.
Variance.black <- var(x.ran_HVLB) + var(y.ran_HVLB)

Bias.black
Variance.black

```

## Facilitating Graphical Comparisons 

To compare two different sets of data, it can be helpful to plot them side by side. R picks boundaries for the x and y axis automatically, and these often don't match across different plots. To ease comparison, you will want the axis to line up. 

This example will be helpful for Lab Question #2. 

```{r histNice, exercise = T}
x <- rnorm(50, 2, 1) ## generate some fake data
y <- rnorm(50, 3, 1) ## generate some fake data

sameMinLimitX <- min(c(x, y)) ## take the minimum of all your data
sameMaxLimitX <- max(c(x, y)) ## take the maximum of all your data

testHistX = hist(x) ## make the histogram to see how high the bars are by default
testHistY = hist(y) ## make the histogram to see how high the bars are by default

sameMaxLimitY <- max(c(testHistX$counts, testHistY$counts)) ## take the maximum height of bars


hist(x, xlim = c(sameMinLimitX, sameMaxLimitX), ylim = c(0, sameMaxLimitY), main = "Informative title here", xlab = "Informative x label here", ylab = "Informative y label here")

hist(y, xlim = c(sameMinLimitX, sameMaxLimitX), ylim = c(0, sameMaxLimitY), main = "Informative title here", xlab = "Informative x label here", ylab = "Informative y label here")
```

## Calculating MLEs

We can calculate maximum likelihood estimators for particular distributions using R. Here is a skeleton of what a log-likelihood function looks like in R.

```{r, likFunction, echo = T, eval = F}

# You will first give your function a name.
    # Then specify the necessary parameters (pars) and data (object) for your function.
    # You will open the function ({)
    # Then you will specify the form of your log-likelihood (logl)
    # You need to tell the function what it should return when you use it
    # Note: the function that we will use to optimize our likelihood only minimizes
    #       functions, therefore we will return the negative log-likelihood
    # Finally, close your function (})
    name = function(pars, object){
          logl = loglikelihood function here
          return(-logl)
}

```

## Poisson Likelihood Function

Let $X_1, X_2, ..., X_n \sim^{iid} Poisson(\lambda)$. Then, the log-likelihood function is given by:

$$l(\lambda) = \sum_{i=1}^n x_i ln(\lambda) - n\lambda - \sum_{i=1}^n ln(x_i!)$$.

Since the last term does not include the parameter $\lambda$, it can be safely ignored for optimization. Thus, the kernel of the log-likelihood function is:

$$l(\lambda) \propto \sum_{i=1}^n x_i ln(\lambda) - n\lambda$$.

We can program this function using the following syntax:

```{r, poissonLik, excercise = T}
poisson.lik <- function(lambda, x) {
  logl <- sum(x) * log(lambda) - length(x) * lambda
  return(-logl)
}

```

Once the log-likelihood function has been declared, then the `nlm` (non-linear minimization) function can be invoked. The minimum specification of this command is `nlm(log-likelihood, starting values, data)`. Here, `starting values` is your (vector of) starting value(s) for the optimization of your parameter.

We will now simulate a data set of 1000 observations from a Poisson($\lambda$ = 3.25) and numerically find the maximum likelihood estimate.


```{r, poissonLik2, exercise = T}
# Set the seed so that we all generate the same data set.
set.seed(288)

poiss.data <- rpois(1000, 3.25)

# Note that we are using 1 as the starting value.
out <- nlm(poisson.lik, 1, x = poiss.data)
pois.mle <- out$estimate

pois.mle

```

How did we do? Not too bad considering the true value of $\lambda$ was 3.25. Is this the value that you were expecting? Why or why not?

## Vector Refresher

To create a vector, you use the concatenate function `c()`. To specify the $i$th position of a vector you would use `[i]` on your vector. 

```{r, cat, exercise = T}

pars <- c(1, 2, 3)

pars[2]

```

## Reading in Data Refresher


```{r, readData, echo = T, eval = F}

gamma_data <- read.csv("gamma_data.csv") ## data frame 

gamma_data$x ## vector

```



## Lab Question #1

**a.)** You will now generate darts for the scenarios of low variance/low bias (color red), low variance/high bias (color blue), high variance/high bias (color orange). For each set of darts, you will calculate bias and variance using similar names. Your answer in your Rmd document should be of the following form:


```{r, lvlb, exercise = T}
# Initiate vectors for (x, y) coordinates
x.ran_HVLB <- y.ran_HVLB <- NULL
x.ran_LVLB <- y.ran_LVLB <- NULL
x.ran_LVHB <- y.ran_LVHB <- NULL
x.ran_HVHB <- y.ran_HVHB <- NULL

## start with a blank target
makeTarget()

for (i in 1:10) {
  # Here, left of the bullseye is negative and right of the bullseye is positive.
  x.ran_HVLB[i] <- runif(1, -2, 2)
  
  x.ran_LVLB <- runif(1, ___, ___)
  x.ran_LVHB <- runif(1, ___, ___)
  x.ran_HVHB <- runif(1, ___, ___)

  # Here, above the bullseye is positive and below the bullseye is negative.
  y.ran_HVLB[i] <- runif(1, -2, 2)
  
  y.ran_LVLB <- runif(1, ___, ___)
  y.ran_LVHB <- runif(1, ___, ___)
  y.ran_HVHB <- runif(1, ___, ___)

  # Draw the dart at the (x,y) coordinates from the previous draw
  points(x.ran_HVLB, y.ran_HVLB, pch = "X", cex = 2, col = "black")
  points(x.ran_LVLB, y.ran_HVLB, pch = "X", cex = 2, col = ___)
  points(x.ran_LVHB, y.ran_HVLB, pch = "X", cex = 2, col = ___)
  points(x.ran_HVHB, y.ran_HVLB, pch = "X", cex = 2, col = ___)
  
  
  Bias.black <- mean(x.ran_HVLB) + mean(y.ran_HVLB)
  Variance.black <- var(x.ran_HVLB) + var(y.ran_HVLB)
  
  Bias.red <- mean(___) + mean(___)
  Variance.red <- var(___) + var(___)
  
  Bias.blue <- mean(___) + mean(___)
  Variance.blue <- var(___) + var(___)
  
  Bias.orange <- mean(___) + mean(___)
  Variance.orange <- var(___) + var(___)

}

# Legend for bias of each scenario
legend("topleft", title = "Bias", legend=round(c(Bias.black, Bias.red, Bias.blue, Bias.orange), 2), col = c("black", "red", "blue", "orange"), pch="X")	

# Legend for variance of each scenario
legend("topright", title = "Variance", legend = round(c(Variance.black, Variance.red, Variance.blue, Variance.orange), 2), col = c("black", "red", "blue", "orange"), pch="X")	
```

**b.)** In 1 - 2 sentences, explain why it is desirable to have estimators that are unbiased with low variance.

## Lab Question #2 

Let $X_1, X_2, ... X_n \sim^{iid} Uniform(0,\theta)$. Recall that $\hat{\theta}_{MOM} = 2 \bar{X}$ and $\hat{\theta}_{MLE*} = \frac{n+1}{n} X_{(n)}$ are two unbiased estimators for $\theta$.

We are going to run a simulation to see how these two estimators perform.

**a.)** Write code based on this pseudocode:

- Draw random samples of size 25 from a Uniform(0,12) distribution.

- For each sample, compute $2\bar{x}$ and $26/25 \times max\{x_1, x_2, ..., x_n\}$ and record those values. 
- Repeat this 1000 times.

**b.)** Create two histograms of the 1000 replications, one for each estimate. Make sure that your histograms have the same x and y axes for comparison. Make sure you include informative x and y axis labels and a title.

**c.)** What is the statistical term of the distributions estimated in the histograms?

**d.)** Which estimator exhibits the smallest amount of variability? Is that what you expected? Why or why not?

## Lab Question #3

Recall that on HW #9, I asked you to write down (but not solve) the equations that you would need to maximize to find the MLEs for $(\alpha, \beta)$ from a Gamma distribution. These partial derivatives are not something that we would want to do by hand (taking the derivative of the gamma function is not too fun), so instead, we will find the MLEs numerically.

**3a)** Write the code for gamma.lik, the log-likelihood function of a Gamma$(\alpha, \beta)$. Note, that this function will now have a vector for the `pars` argument. 

**3b)** Read in "gamma_data.csv" from Moodle (make sure it is in the same folder as your Rmd file) and optimize this data with starting values of (1, 1) for $(\alpha, \beta)$.

Note: if you use `read.csv()`, R will read in the data as a data frame and your likelihood function will not be able to handle an object of that class. To avoid this issue, you will have to pull off the `x` column from the data (`dataname$x`) and pass that into your likelihood function.

**3c)** What are your MLEs for $(\alpha, \beta)$?


* * *

This lab was created by A. Flynt and was adapted and learnr-ified by S. Stoudt.